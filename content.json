{"pages":[{"title":"about","text":"개인 블로그 통계 머신러닝 딥러닝 기타","link":"/about/index.html"}],"posts":[{"title":"LIME","text":"XAI에 대한 개념과 알고리즘의 하나인 LIME 설명 1. 머신러닝 공통 기능 머신러닝에 쓰이는 많은 모형들은 공통적으로 크게 2가지 기능을 갖고있다. 예측력 설명력 의사결정나무이 오랜시간동안 사랑을 받아온 이유는 이 2가지를 모두 만족시켜주는 모형이기 때문으로 설명할 수 있다. (어느정도 부족한 예측력을 상쇄하는 높은 설명력) 2. 딥러닝 설명력 최근 딥러닝의 등장으로 예측력에 대한 부분은 과거에 비해 상당히 개선된 것을 확인할 수 있다. 하지만, 딥러닝 모형의 설명력은 다른 모형에 비해서 낮을 수 밖에 없는 구조를 갖고있어서 해석에 대한 한계점을 갖고있다. 딥러닝의 경우 W를 통해 강한 노드와 약한 노드를 구분하게 되어 자동적으로 변수선택이 된다. 근데, 왜 해석이 어려운걸까? (개인적으로) XAI에서 말하는 딥러닝의 해석은, 단순히 어떤 변수가 중요하다가 아닌, 회귀 계수와 같이 어떤 변수가 얼만큼 중요한지와 같은 정량적인 해석을 원하는 듯. 딥러닝의 경우 layer가 쌓일 수록 이 Beta를 딱 하나의 상수로 정의할 수 없기때문에 해석이 어려운 것으로 생각함. 3. XAI XAI는 높은 예측력을 가진 딥러닝 모형에 부족한 설명력을 강화하는데 그 목적을 두고있다. 단, XAI도 결국 과거 모형들을 대상으로 연구되었던 변수선택법이나 모형선택법 등과 유사한 형태의 알고리즘이라고 할 수 있다. XAI = important variable = variable selection 4. Concept of Variable Importance 기존의 머신러닝에서 변수 중요도를 확인하는 가장 간단한 방법은 다음과 같다. 변수 $x, y$ 와 모형 $f$가 있다고 했을 때, $$\\epsilon_1 = f(x,y) - f(x) \\\\\\epsilon_2 = f(x,y) - f(y)$$ 두 수식이 있다고 가정한다면, $f(x, y)$ 는 모형에 모든 변수를 넣었을 때의 결과를 나타내고,$f(x)$ 와 $f(y)$는 변수 하나씩만 넣었을 때의 결과를 나타낸다.만약에 $x$ 는 매우 중요한 변수고, $y$ 는 그렇지 않은 변수라고 가정한다면,$f(x, y)$ 의 결과와 $f(x)$ 는 매우 유사할 것이며,$f(x,y)$ 와 $f(y)$ 는 매우 다른 값이 나올 것이다.따라서, $\\epsilon_1$ 이 $\\epsilon_2$ 보다 작을 것이라고 결론을 내릴 수 있다. 이처럼, 어떤 변수가 중요한지 아닌지 알아보기 위해선, 그 변수의 유무에 따른 모형의 결과 값이 얼만큼 차이나는지를 통해 확인해볼 수 있다. 5. Lime (Local Interpretable Model-agnostic Explanation) Lime 또한 variable importance의 concept을 활용한 알고리즘이다. 단, 기존의 모형과 다르게 딥러닝에 들어가는 변수(Link 자료에선 feature로 명명) 가 너무 많은 경우 계산량이 너무 많기 때문에, 해석하고 싶은 변수를 K개로 정해놓고**(Local)** 그 K개의 변수 중 어떤 변수가 중요한지를 계산하는 방식을 사용한다. 데이터 설명$$\\text{원 데이터: } x \\in R^d \\\\\\text{interpretable data: } x’ \\in R^{d’} \\\\f \\text{ : 기존 모형 - 해석 대상} R^d→R \\\\g\\text{ : 기존 모형을 해석하기 위한 모형 - 해석에 용이한 모형} R^{d’}→R$$ x의 경우, 원 데이터를 말한다. 예) I hate the dog x’의 경우, x의 변형 데이터를 말한다. 예) I hate / i hate dog / hate the dog / i the dog … z’ 의 경우, x’ 중에서 특정 변수를 포함한 데이터를 말한다. 예) hate 기준→ i hate / i hate dog / hate the dog … / i the dog(hate가 없음. z’에서 탈락) z의 경우, z’를 x와 같은 차원으로 맞춘 데이터. 예) i hate (2개) → i hate (4개) Object function Lime 의 Object function은 아래와 같다. $$\\xi(g) = \\arg\\min_{g \\in G} L(f,g,\\pi_z(x)) + \\Omega(g) \\\\L(f,g,\\pi_z(x)) = \\sum \\pi_z(x)(f(z)-g(z’))^2 \\\\\\Omega(g) \\text{ : g 모형 복잡도 - g 모형 변수 개수} \\\\\\pi_z(x) \\text{ : 유사도 - exponential kernel}$$ Object function은 크게 Loss function L과 penalty term Ω로 구성된다. L의 경우, 해석 대상인 f와 f를 해석하려하는 g의 차이를 나타낸다. 만약 g가 해석 가능하면서 f랑 비슷한 결과를 내놓는다면, g를 통해 f를 해석할 수 있다. Ω의 경우, g의 복잡도를 낮추는 역할을 한다. 위의 L을 통해 f와 비슷한 모형 g를 찾았는데, 그 g 또한 매우 복잡하다면 g를 통해 f를 해석하는 것은 어렵게 된다. 모형 복잡도는 초기에는 사용자가 임의의 상수 K를 정하도록 하였으며, 이후 Lasso 를 통해 최적의 K를 찾을 수 있도록 연구되었다 Lasso 또한 penalty term λ를 정해야하는 문제가 있다. ⇒ Lime package에선 0.1 사용 summary: x → x’ → z’ → (z’ == 1) → z → f(z) / g(z’) → L(f, g, pi) → Ω → argmin(L+Ω) Reference Blog: 머신러닝 모델의 블랙박스 속을 들여다보기 : LIME","link":"/2020/10/07/ml/Lime/"},{"title":"newsgroup","text":"Scikit-learn을 이용하여 newsgroup을 분석하는 process Workflow Load data Data transformation Pipeline을 이용한 모델 학습 및 예측 Load data데이터 다운 및 읽기 1from sklearn.datasets import fetch_20newsgroups 123cats = ['alt.atheism', 'sci.space']newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)newsgroups_test = fetch_20newsgroups(subset='test', categories=cats) 1234X_train = newsgroups_train.dataX_test = newsgroups_test.datay_train = newsgroups_train.targety_test = newsgroups_test.target 1234len(X_train),\\len(X_test),\\y_train.shape,\\y_test.shape (1073, 713, (1073,), (713,)) 12print(X_train[0])print(&quot;[Label: {}]&quot;.format(y_train[0])) From: bil@okcforum.osrhe.edu (Bill Conner) Subject: Re: Not the Omni! Nntp-Posting-Host: okcforum.osrhe.edu Organization: Okcforum Unix Users Group X-Newsreader: TIN [version 1.1 PL6] Lines: 18 Charley Wingate (mangoe@cs.umd.edu) wrote: : : &gt;&gt; Please enlighten me. How is omnipotence contradictory? : : &gt;By definition, all that can occur in the universe is governed by the rules : &gt;of nature. Thus god cannot break them. Anything that god does must be allowed : &gt;in the rules somewhere. Therefore, omnipotence CANNOT exist! It contradicts : &gt;the rules of nature. : : Obviously, an omnipotent god can change the rules. When you say, &quot;By definition&quot;, what exactly is being defined; certainly not omnipotence. You seem to be saying that the &quot;rules of nature&quot; are pre-existant somehow, that they not only define nature but actually cause it. If that's what you mean I'd like to hear your further thoughts on the question. Bill [Label: 0] Data Transformation1from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer 12vect = CountVectorizer()tfidf = TfidfTransformer() Pipeline을 이용한 학습 구성필요한 모델 Estimator 인스턴트화123from sklearn.metrics import f1_scorefrom sklearn.model_selection import cross_val_scorefrom sklearn.svm import LinearSVC 1clf = LinearSVC() PipelinePipeline은 모델의 학습 과정에 포함되는 몇가지의 단계를 단순화해주는 기능이다. 이를 통해, 최종 모델을 위한 재학습이나 Cross-validation 등을 위해 학습을 반복하는 상황에서, Pipeline을 이용하면 이를 쉽고 간편하게 진행할 수 있다. 1from sklearn.pipeline import Pipeline 12345pipeline = Pipeline([ ('vect',vect), ('tfidf',tfidf), ('clf',clf)]) Test set 검증123456# now train and predict test instancespipeline.fit(X_train,y_train)y_preds = pipeline.predict(X_test)# calculate f1f1_score(y_test, y_preds, average='micro') 0.9747545582047685 Cross validationCV를 위해서는 데이터의 변환, 학습, 평가를 k번 만큼 진행해야 한다. 따라서 위에서 만든 pipeline을 이용하여 CV를 사용한다 1import numpy as np 12X = np.append(X_train, X_test)y = np.append(y_train, y_test) 123scores = cross_val_score(pipeline,X,y,cv=5,scoring='f1_micro')scores array([0.99162011, 0.99159664, 1. , 0.9859944 , 0.99439776]) 1scores.mean() 0.9927217814500102 Grid search모델 최적화를 위한 파라미터 탐색에 사용되는 Grid Search 방법을 적용하기 위해서도 반복적인 학습이 필요하다. Grid Search를 통한 파라미터 최적화도 pipeline을 통해 쉽게 구성할 수 있다 파라미터 최적화를 위해서, 각 파이프라인을 구성하는 함수들에서 최적화 하고싶은 파라미터들의 범위를 설정한다| 123456789101112param_grid = [ { 'vect__max_df':[0.8,0.9,1.0], 'clf__penalty':['l2'], 'clf__dual':[True,False] }, { 'vect__max_df':[0.8,0.9,1.0], 'clf__penalty':['l1'], 'clf__dual': [False] }] GridSearchCV는 Cross validation 에서 각 k마다 Grid Search를 통해 파라미터를 최적화하는 방법이다 1from sklearn.model_selection import GridSearchCV 12grid = GridSearchCV(pipeline, cv=5, param_grid=param_grid, scoring='f1_micro')grid.fit(X_train,y_train) GridSearchCV(cv=5, error_score=nan, estimator=Pipeline(memory=None, steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict', dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content', lowercase=True, max_df=1.0, max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None, stop_words=None, strip_accents=None, token_pattern='(?u)... penalty='l2', random_state=None, tol=0.0001, verbose=0))], verbose=False), iid='deprecated', n_jobs=None, param_grid=[{'clf__dual': [True, False], 'clf__penalty': ['l2'], 'vect__max_df': [0.8, 0.9, 1.0]}, {'clf__dual': [False], 'clf__penalty': ['l1'], 'vect__max_df': [0.8, 0.9, 1.0]}], pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring='f1_micro', verbose=0) 최적 파라미터 1print(&quot;Best: %f using %s&quot; % (grid.best_score_, grid.best_params_)) Best: 0.994414 using {'clf__dual': True, 'clf__penalty': 'l2', 'vect__max_df': 0.8} 전체 실험결과 12345means = grid.cv_results_['mean_test_score']stds = grid.cv_results_['std_test_score']params = grid.cv_results_['params']for mean, stdev, param in zip(means, stds, params): print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param)) 0.994414 (0.003479) with: {'clf__dual': True, 'clf__penalty': 'l2', 'vect__max_df': 0.8} 0.994414 (0.003479) with: {'clf__dual': True, 'clf__penalty': 'l2', 'vect__max_df': 0.9} 0.993484 (0.004742) with: {'clf__dual': True, 'clf__penalty': 'l2', 'vect__max_df': 1.0} 0.994414 (0.003479) with: {'clf__dual': False, 'clf__penalty': 'l2', 'vect__max_df': 0.8} 0.994414 (0.003479) with: {'clf__dual': False, 'clf__penalty': 'l2', 'vect__max_df': 0.9} 0.993484 (0.004742) with: {'clf__dual': False, 'clf__penalty': 'l2', 'vect__max_df': 1.0} 0.972984 (0.009003) with: {'clf__dual': False, 'clf__penalty': 'l1', 'vect__max_df': 0.8} 0.972984 (0.011150) with: {'clf__dual': False, 'clf__penalty': 'l1', 'vect__max_df': 0.9} 0.974853 (0.011596) with: {'clf__dual': False, 'clf__penalty': 'l1', 'vect__max_df': 1.0} 최적 파라미터를 통한 Test accuracy. 파라미터를 최적화하지 않았을 때보다 근소하게 accuracy가 증가했다. 12345678# now train and predict test instances# using the best configspipeline.set_params(clf__penalty='l2',vect__max_df=0.9,clf__dual=True)pipeline.fit(X_train,y_train)y_preds = pipeline.predict(X_test)# calculate f1f1_score(y_test, y_preds, average='micro') 0.9761570827489481 1","link":"/2020/10/07/ml/newsgroup/"},{"title":"Unsupervised Data Augmentation","text":"Google에서 발표한 Semi-supervised learning 방법 중 하나인 UDA 설명 Background: Loss for Semi-supervised learning Consistency / smoothness enforcing 목적함수 컨셉 기존 데이터 x에 적절한 noise(예 - 가우시안 노이즈)를 더하여 새로운 데이터 x*를 생성 x의 모델 예측값 y와 x의 모델 예측값 y의 분포가 유사(일치)하도록 학습$$x + \\epsilon \\rightarrow x^* , \\epsilon \\sim N(0,1) \\\\x \\rightarrow \\text{model} \\rightarrow \\hat{y} \\\\x \\rightarrow \\text{model} \\rightarrow \\hat{y} \\\\\\hat{y} \\leftarrow \\text{consistency} \\rightarrow \\hat{y}$$ Data Augmentation 목적함수$$\\underset{\\theta}{\\mathrm{min}} \\tau_{DA}(\\theta)= \\underset{x,y \\in L}{E}\\text{ }\\underset{\\hat{x} \\sim q(\\hat{x}|x)}{E}[-logP_{\\theta}(y|\\hat{x})]$$ x: original data $\\hat{x}$: augmented data $y^*$: ground truth 설명 목적함수 T는 증강된 데이터 $\\hat{x}$ 에 대한 ground truth $\\hat{y}$의 조건부 확률 값이 1에 가까운 방향으로 학습하도록 설정 Unsupervised Data Augmentation(UDA) 개념 Consistency / smoothness enforcing 과 Data augmentation을 결합한 개념 Supervised crossentropy loss와 Unsupervised consistency loss로 구성되어 있음 세부 설명 Unsupervised consistency loss $$ \\underset{\\theta}{\\mathrm{min}} \\tau_{UDA}(\\theta) = \\underset{x \\in U}{E} \\text{ } \\underset{\\hat{x} \\sim q(\\hat{x}|x)}{E}[D_{KL}(P_{\\tilde{\\theta}}(y|x) || P_{\\theta}(y|\\hat{x}))] $$ 실제 데이터 $x$를 통해 예측된 y의 분포 $P_{\\tilde{\\theta}}(y|x)$ 증강 데이터 $\\hat{x}$를 통해 예측된 y의 분포 $P_{\\theta}(y|\\hat{x})$ 두 분포의 K-L divergence가 가까워지는(Consistency) 방향으로 학습 Augmentation Method Image: AutoAugment Text Back translate 예) (1)한국어 -&gt; (2)영어 -&gt; (3)한국어 (!) 한국어와 (3) 한국어의 결과가 다를 수 있다는 아이디어를 사용 TF-IDF based word replacing 특정 단어를 TF=IDF 값이 유사한 다른 단어로 치환 Training Signal Anealing $$ \\underset{\\theta}{min} \\frac{1}{Z} \\underset{x,y \\in B}{\\sum} [-logP_{\\theta}(y|x)I(P_{\\theta}(y|x) \\lt \\tau_t)] $$ 데이터 x에 대한 y의 확률값이 특정값 $\\tau$보다 작을때에만 supervised cross-entropy loss로 사용하는 방법 $\\tau$의 경우 학습이 진행됨에 따라 점점 증가한다 초기에는 확률이 작은(=모호한 데이터)를 학습 데이터로 활용 후기에는 확률이 큰(=확실한 데이터)를 학습 데이터로 활용 초반에 확실한 데이터를 배제하고 학습함으로써, overfitting을 방지하는 효과를 보여준다 ReferencePaperYoutube","link":"/2020/10/07/ml/UDA/"},{"title":"Hexo를 이용하여 git 블로그 만들기1","text":"Hexo를 이용하여 기본적인 블로그를 만드는 방법 환경 설정Hexo를 이용하여 blog를 만들기 위해서는 Node.js와 Git 두가지가 필요하다. 아래 링크를 통해 각 사이트에서 권장하는 version을 다운받고, default option 으로 설치한다 Node.js Git Hexo 설치Node.js 설치 후 Window 검색창에서 Node.js command prompt를 검색 및 실행 후 다음의 명령어를 입력하여 Hexo를 설치한다. 1npm install -g hexo-cli Hexo 설치 후 원하는 Local directory에 다음 명령어를 실행한다. 1hexo init &lt;폴더명&gt; hexo 기본 폴더는 Local directory 하위에 생성이 된다.hexo 기본 폴더에는 기본적인 페이지 및 기능을 포함하고 있는 파일들이 있다. hexo 기본 폴더에서 다음과 같은 명령어를 실행하여 로컬 서버를 실행한다. 1hexo server 브라우저에서 http://0.0.0.0:4000/ 혹은 http://localhost:4000/ 로 접속하면 블로그를 확인할 수 있다. 새 Post 만들기","link":"/2020/10/07/hexo/hexo1/"},{"title":"Hexo를 이용하여 git 블로그 만들기2","text":"Hexo theme 중 하나인 Icarus를 블로그에 적용하는 방법 Icarus 다운 및 설치git 블로그를 위한 theme을 제공하는 Jekyll처럼, Hexo에도 사전제작된 theme들이 있다. 여러가지 theme중에서 Icarus theme의 설치 방법을 소개한다. 아래의 링크를 통해 Icarus를 다운받는다. Icarus 다운 다운받은 Icarus 파일은, 이전에 hexo를 설치했던 local directory내의 폴더명&gt;theme 아래에 icarus라는 폴더이름으로 위치시킨다. 기존에 있던 폴더 landscape는 삭제한다. 폴더명 &gt; theme &gt; Icarus 이후 서버 명령어를 통해 Icarus theme가 적용되었는지 확인할 수 있다. 1hexo server 배포http://0.0.0.0:4000/ 혹은 http://localhost:4000/ 를 통해 local에서 확인한 Icarus theme를 github에 배포할 수 있다. 먼저 config.yml에 자신의 github 정보를 입력한다. 1234deploy: type: git repo: https://github.com/github_id/github_id.github.io branch: master 배포를 위해선 hexo-deployer-git 플러그인이 필요하다. 아래의 명령어를 통해 설치를 진행한다. 1npm install --save hexo-deployer-git 다음으로는 배포할 정적파일들을 생성한다. 아래의 명령어를 수행한 후, public이라는 폴더가 새로 생성되는 것을 확인할 수 있다. 123hexo generate또는hexo g 마지막으로, 배포 명령어를 실행하여 github에 정적파일을 배포한다. 123hexo deploy또는hexo d","link":"/2020/10/07/hexo/hexo2/"},{"title":"Delta method","text":"Delta method 설명 Delta method theorem Theorem: Delta method Let $\\phi:D_\\phi \\in R^k \\rightarrow R^m$ be a map defined on a subset of $R^k$ and differentiable at $\\theta$. Let $T_n$ be random vectors taking their values in the domain of $\\phi$. If $r_n(T_n - \\theta) \\rightarrow T$ for numbers $r_n \\rightarrow \\infty$ ,then $r_n(\\phi(T_n) - \\phi(\\theta)) \\rightarrow \\phi’_{\\theta}(T)$. Moreover, the difference between $r_n(\\phi(T_n) - \\phi(\\theta))$ and $\\phi’_{\\theta}(r_n(T_n - \\theta))$ converges to zero in probability. Explain There exists $T_n$ for a parameter $\\theta$ and some known function $\\phi$. Our goal is how asymptotic properties of $\\phi(T_n)$ follow from those of $T_n$ $T_n = (T_{n1},…,T_{nk})$ $\\phi: R^k \\rightarrow R^m$, differentiable at $\\theta$ If $\\sqrt{n}(T_n-\\theta)$ converges weakly to distribution Y, then we expect that $\\sqrt{n}(\\phi(T_n) - \\phi(\\theta))$ also follows distribution $\\phi’(\\theta)Y$ If $\\sqrt{n}(T_n-\\theta) \\rightarrow Y$, $\\sqrt{n}(\\phi(T_n) - \\phi(\\theta)) \\rightarrow \\phi’(\\theta)Y$ Example There exists $X_i$ which is iid for all $i$. then, we expect $\\sqrt{n}(\\bar{X}- \\mu) \\rightarrow N(0, \\sigma^2)$ by C.L.T Suppose function $g(x) = 1/x$, then we know $g(\\bar{X})$ also follows normal distribution by Delta method $g’(x) = -1/x^2$ $g’(\\mu) = -1/\\mu^2$ $g’(\\mu)N(0, \\sigma^2) = 1/\\mu^2 N(0, \\sigma^2)$ $\\therefore \\sqrt{n}(1/\\bar{X}- 1/\\mu) \\rightarrow -1/\\mu^2 N(0, \\sigma^2)$ Generalization of delta method : what if var =0? Gx = $x^2$ Mu = 0 Reference [1] Van der Vaart, Aad W. Asymptotic statistics. Vol. 3. Cambridge university press, 1998.","link":"/2020/10/07/stat/delta-method/"},{"title":"Markov Chain Monte Carlo","text":"Monte carlo Monte carlo(MCMC) Markov chainDefinition a stochastic process in which future state are independent of past states given the present state Markov property $p(\\theta^{t+1}|\\theta^{1},\\theta^{2},…,\\theta^{t}) = p(\\theta^{t+1}|\\theta^{t})$ Markov chain is a bunch of draws of θ that are each slightly dependent on the previous one. Transition kernel (3 x 3) $$P = \\begin{Bmatrix}P(A^{t+1}|A^{t}) &amp; P(B^{t+1}|A^{t}) &amp; P(C^{t+1}|A^{t})\\\\P(A^{t+1}|B^{t}) &amp; P(B^{t+1}|B^{t}) &amp; P(C^{t+1}|B^{t})\\\\P(A^{t+1}|C^{t}) &amp; P(B^{t+1}|C^{t}) &amp; P(C^{t+1}|C^{t})\\end{Bmatrix}$$ Example Suppose the state space are (Rain, Sunny, Cloudy) and whether follows a Markov process (what is this implication?). Suppose the probability transitions given today is rainy are P(Rain tomorrow | Rain today) = 0.5 P(Sunny tomorrow | Rain today) = 0.25 P(Cloudy tomorrow | Rain today) = 0.25 Transition Matrix $P=\\begin{Bmatrix} 0.5 &amp; 0.25 &amp; 0.25\\\\ 0.5 &amp; 0 &amp; 0.5\\\\ 0.25 &amp; 0.25 &amp; 0.5 \\end{Bmatrix} = \\begin{Bmatrix} P(R|R) &amp; P(S|R) &amp; P(C|R)\\\\ P(R|S) &amp; P(S|S) &amp; P(C|S)\\\\ P(R|C) &amp; P(S|C) &amp; P(C|C) \\end{Bmatrix}$ Suppose today is sunny. What is the expected weather two days from now? seven days? $\\pi^{0} = (0, 1, 0)$ → $\\pi^{2} = \\pi^{0}*P^{2}$ ⇒ stationary distribution Properties for Markov chain Stationary Distribution $\\pi = \\pi * P$ detailed balance Ergodic Theorem Problem: draws → not indendent if Markov chain is ergodic(aperiodic, irreducible, and positive recurrent), then we can draw independent sample using MCMC Markov Chain Monte CarloMCMC Monte Carlo: Simulation with Random Sampling MCMC is a class of methods in which we can simulate draws that are slightly dependent and are approximately from a (posterior) distribution. MCMC application Metropolist-Hastings Monte Carlo Sampling + 기각법Markov Chain Monte Carlo 샘플링의 마법 Gibbs sampler","link":"/2020/10/09/stat/mcmc/"},{"title":"Monte Carlo Integration and Variance Reduction","text":"Monte carlo 적분과 분산 축소(efficiency) 몬테 카를로 방법(Monte Carlo Methods) 정의(from 나무위키)동명의 카지노에서 따온 이름을 가진, 무작위 추출된 난수를 이용하여 함수의 값을 계산하는 통계학의 방법. 최적화, 수치적분, 확률분포로부터의 추출 등에 쓰인다. 의미난수 생성을 통해서 주어진 계산을 하면 모두 몬테 카를로 방법이라고 말할 수 있다. 난수를 무한히 생성할 수 있다면, 난수를 통해 계산된 값은 실제 모수에 수렴한다고 할 수 있다(by SLLN) Monte Carlo Integration with Sampling 적분 방법어떤 주어진 함수 g(x)에 대해 적분값은 Analytical solution: 수식을 통해 직접 계산 가능한 방법. 사용할 수 있다면 정확한 결과를 도출. Numerical solution: 수식을 사용할 수 없는 경우, 근사를 통해 결과를 도출. 으로 계산 가능함 Sample mean Monte Carlo method다음과 같이 주어진 함수 g(x)를 주어진 구간 (a, b)에서 적분한 결과를 θ라고 한다면, $$\\theta = \\int_{a}^{b} g(x)dx \\dots \\text{ (1)}$$ 라고 풀 수 있다. 여기서, $$g(x) = 5x^4 + 3x^2$$ 와 같이 계산하기 쉬운 값이면, 직접 계산을 통한 analytical solution을 얻을 수 있다. 하지만 g(x)가, $$g(x) = \\frac{e^x}{1-x^2}$$ 와 같은 형태라면 analytical solution을 얻기 쉽지 않다. 만약 θ를 우리가 알고있고 계산할 수 있는 f(x) 의 형태로 표현한다면, $$\\theta = \\int_{a}^{b} g(x)dx = \\int_{a}^{b} \\frac{g(x)}{f(x)}f(x)dx = E_f[g(x)] \\dots \\text{ (2)}$$ 로 표현할 수 있다. 수식 (1)과 (2)의 큰 차이는, (1)에서의 x는 문자 그대로 변수 (2)에서의 x는 함수 f(x)를 분포로 따르는 확률변수 물론 여전히 (2)를 계산하는 것은 쉽지않다. 하지만, 대수의 법칙(Strong Law of Large Number)를 이용하게 되면, (2)를 평균의 문제로 근사할 수 있게 된다. $$\\theta = E_f[g(x)] \\to \\hat \\theta = \\frac{1}{m} \\sum_{i=1}^{m} g(x_i) \\text{ as } m \\to \\infty \\text{ by S.L.L.N \\dots (3)}$$ f(x)를 잘 알려진 분포로 가정한다면, → f(x)로 부터 x를 무수히 생성한다 → 수식 (3)에 생성한 x를 대입하여 결과를 얻는다. → 얻은 결과는 수식 (1)의 결과에 수렴한다고 할 수 있다. → 수식 (3)을 통해 (1)의 결과값을 계산할 수 있다. hit-or-miss Hit or Miss Monte Carlo Integration Variance reduction Variance of θθ를 구할 수 있는 방법은 위의 sample mean monte carlo와 hit-or-miss 뿐만 아니라 다양한 방법이 존재하기 떄문에, 결과값에 차이가 존재한다. 또한, 사용한 난수의 수에 따라서 결과값에 차이가 생길 수 있다. 따라서, 우리가 어떤 방법을 통해 θ를 구한다고 한다면, 가장 안정적인 θ값을 계산하는 방법을 사용하는 것이 좋다. 다시 말하면, 분산이 가장 작은 θ 계산 방법을 사용하는 것이 좋다. 혹은 θ의 분산을 줄이는 방향으로 θ를 추정하는 것이 좋은 계산 방법이다. Reduction methods2.1 Antithetic variate condition: f(x) &amp; g(x) are negatively correlated $$\\hat \\theta_{\\hat c} = \\frac{\\hat \\theta_1 + \\hat \\theta_2}{2}$$ 2.2 Control variate condition: f(x) &amp; g(x) are just correlated $$\\hat \\theta_{\\hat c} = g(x) + c(f(x)+ \\mu_f)$$ 2.3 Antithetic variate as Control variate The antithetic variate estimator of the previous section is actually a special case of the control variate estimator. Antithetic variate = Control variate with θ1, θ2 are negatively correlated 2.4 Importance sampling f(x)가 g(x)와 근사 할 수록 분산 축소가 클 것이다. 이 때, f(x)를 importance function으로 부른다. f(x)가 g(x)와 근사할 경우 → Importance sampling f(x)가 일반적인 uniform일 경우 → sample mean Monte Carlo(= uniform sampling) 2.5 Stratified sampling 층화 추출법 $$Var(\\hat \\theta_k(m_1,\\dots,m_k)) &lt; Var(\\hat \\theta)$$ $$m = m_1 +\\dots + m_k$$","link":"/2020/10/08/stat/monte-carlo/"},{"title":"Random number generation","text":"난수 생성 설명 정의 어떤 분포에서 원하는 값을 생성하는 것. 예) 평균이 0이고 분산이 1인 표준 정규분포에서 난수 하나를 생성하는 것 예) 확률이 0.5인 베르누이 분포에서, 동전을 던졌을 때 어떤 면이 나올지를 생성하는 것 의미/필요성 임의의 함수 f(x)의 global optimum을 찾는 최적화 문제를 푼다고 할 때, 임의의 상수 K에 대해 $e^{Kf(x)}$ 에 비례하는 확률 분포 p(x)가 존재한다면, 이 확률분포 p(x)로부터 샘플을 쉽게 얻을 수 있다. 만약 K 에 큰 값을 부여한다면, f(x)의 값이 높은 x일 수록 상대적으로 샘플링 빈도가 매우 높아질 것임. 따라서 K를 무한대에 가깝게 설정하면, 결국 쉽게 얻어진 샘플들은 global optimum으로 수렴하게 된다. 또한 x를 subset으로 f를 subset에 속한 원소의 합으로 치환 =&gt; Subset sum problem 적용 일반적으로, 우리가 모르는 확률분포에서 알려진 방법으로 난수를 생성 할 수 없을 때 사용 어떤 분포를 따르는 모든 함수(고차원 등) 의 최적화 문제 (subset sum problem) 확률분포를 통해 전체 → 일부만 확인하여 문제를 해결 적분 대수의 법칙을 통해 고차원의 적분 계산 을 계산할 수 있다. 난수 생성 방법 역변환법 설명 값이 (0, 1) 사이인 누적분포함수 CDF의 성질을 이용하여 난수를 생성하는 방법 방법 목표: 분포함수 $f$ 에서 난수 생성 알려지지 않은 형태의 분포함수 $f$ 가 존재 ($x \\sim f(.)$) U(0, 1)을 따르는 균일분포 난수 u를 무한대로 생성할 수 있음 ($u \\sim U(0, 1)$) 누적분포함수 F(x)를 계산. $u = F(x)$ 역함수를 계산. $x = F^{-1}(u)$ 생성한 균일분포 난수 u를 위 식에 대입 → 난수 x 생성 완료 그림 기각법 설명 두 확률분포의 비교를 통해, 더 그럴듯한? 난수를 추출하는 방법 방법 목표: 분포함수 $p$ 에서 난수 생성 분포함수 $p$ 가 존재 ($x \\sim p(.)$) 분포함수 $q$ 가 존재 ($y \\sim q(.)$), 난수 생성이 가능한 분포) $q(.)$에서 난수 y 생성 $h(y) = min[\\frac{f(y)}{g(y)}, 1]$ 계산 U(0, 1)을 따르는 균일분포 난수 u를 생성 $h(y) &gt; u$ → y 를 난수로 채택 그림 Reference Markov Chain Monte Carlo 샘플링의 마법 P, NP문제와 co-NP, NP-난해(NP-Hard), NP-완전(NP-complete) 개념 정리 연속적 확률난수 생성 기각 샘플링 (Rejection Sampling)","link":"/2020/10/07/stat/random-number-generation/"},{"title":"Titanic","text":"Scikit-learn을 이용하여 Kaggle의 Titanic를 분석하는 process Workflow stages 문제 정의 Question or problem definition. 데이터 수집 Acquire training and testing data. 데이터 전처리 Wrangle, prepare, cleanse the data. 데이터 탐색(컬럼 탐색 분석, 패턴 분석 등) Analyze, identify patterns, and explore the data. 데이터 모델링 Model, predict and solve the problem. 결과 생성 및 시각화 Visualize, report, and present the problem solving steps and final solution. Workflow goals 데이터 전처리 데이터 변형 Converting 데이터 정합성 확인? Completing 중요 변수 선택 Correlating 모델 성능 Correcting 시각화 Charting 문제정의 Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not. - training data를 통해 생사여부를 판단할 수 있는 모델을 완성 - test data를 통해 검증 데이터데이터 수집123456789import pandas as pdimport numpy as npfrom matplotlib import pyplot as pltimport seaborn as snsimport reimport warningsfrom statistics import modewarnings.filterwarnings('ignore')plt.style.use('fivethirtyeight') 123train = pd.read_csv(&quot;data/titanic/train.csv&quot;)test = pd.read_csv(&quot;data/titanic/test.csv&quot;)gender_submission = pd.read_csv(&quot;data/titanic/gender_submission.csv&quot;) 데이터 확인1train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB 1train.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 데이터 탐색 변수 정의 분석 방향 설명 Survived 생사여부 0: No, 1:Yes Pclass 탑승 클래스/등급 클래스별 생존확률, Pclass, Fare 와의 상관관계 Name 이름 Mr나 Miss, Sir 같은 단어가 영향을 미칠것인가 Sex 성별 성별간 생존률 확인 카테고리변경 Age 나이 어릴수록? SibSp 자식 숫자 자식수와 생존 상관관계 Parch 부모 숫자 부모수와 생존 상관관계 Ticket 티켓 넘버 Fare와 Corr 높을 듯 / 비용과 생존의 Corr? Fare 요금 Ticket과 동일 카테고리변경 Cabin Cabin 여부 및 번호 Embarked 탑승 항구 C = Cherbourg, Q = Queenstown, S = Southampton Sex 사망자:생존자 약 6:4의 비율 남성의 경우 여성보다 더 많이 사망 생존에 중요한 변수로 추정 12345678plt.figure(figsize=(15,5))plt.subplot(1,2,1)sns.countplot(train['Survived'])plt.title('Number of passenger Survived');plt.subplot(1,2,2)sns.countplot(x=&quot;Survived&quot;, hue=&quot;Sex&quot;, data=train)plt.title('Number of passenger Survived') Text(0.5, 1.0, 'Number of passenger Survived') Pclass 탑승 클래스 금액이 저렴한 3등급 탑승객들의 수가 과반수 이상 사망자수 또한 3등급에서 많이 나옴 12345678910plt.figure(figsize=(15,5))plt.style.use('fivethirtyeight')plt.subplot(1,2,1)sns.countplot(train['Pclass'])plt.title('Count Plot for PClass');plt.subplot(1,2,2)sns.countplot(x=&quot;Survived&quot;, hue=&quot;Pclass&quot;, data=train)plt.title('Number of passenger Survived'); Age 탑승자들의 나이 분포는 주로 20~30대에 몰려있음 12plt.figure(figsize=(15, 3))train['Age'].plot(kind='hist', bins = 80) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb36983630&gt; 12345678910111213# set plot sizeplt.figure(figsize=(15, 3))# plot a univariate distribution of Age observationssns.distplot(train.loc[train[&quot;Age&quot;] &gt; 0, 'Age'], kde_kws={&quot;lw&quot;: 3}, bins = 80)# set titles and labelsplt.title('Distrubution of passengers age',fontsize= 14)plt.xlabel('Age')plt.ylabel('Frequency')# clean layoutplt.tight_layout() 생존자들의 연령 분포가 사망자들의 연령 분포보다 좀 더 낮은 것을 확인 123456789101112plt.figure(figsize=(15, 3))# Draw a box plot to show Age distributions with respect to survival status.sns.boxplot(y = 'Survived', x = 'Age', data = train, palette=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], fliersize = 0, orient = 'h')# Add a scatterplot for each category.sns.stripplot(y = 'Survived', x = 'Age', data = train, linewidth = 0.6, palette=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], orient = 'h')plt.yticks( np.arange(2), ['drowned', 'survived'])plt.title('Age distribution grouped by surviving status (train data)',fontsize= 14)plt.ylabel('Passenger status after the tragedy')plt.tight_layout() SibSp 자녀 수가 많을 수록 생존한 것을 확인할 수 있음 단, 그 수가 많지는 않음 123456789101112plt.figure(figsize=(15,5))plt.subplot(1,2,1)sns.countplot(train['SibSp'])plt.title('Number of siblings/spouses aboard');plt.subplot(1,2,2)sns.countplot(x=&quot;Survived&quot;, hue=&quot;SibSp&quot;, data=train)plt.legend(loc='right')plt.title('Number of passenger Survived');# clean layoutplt.tight_layout() Embarked123456789101112plt.figure(figsize=(15,5))plt.subplot(1,2,1)sns.countplot(train['Embarked'])plt.title('Number of Port of embarkation');plt.subplot(1,2,2)sns.countplot(x='Survived', hue='Embarked', data=train)plt.legend(loc='right')plt.title('Number of passenger Survived')# clean layoutplt.tight_layout() Fare vs Embarked123sns.catplot(x=&quot;Embarked&quot;, y=&quot;Fare&quot;, kind=&quot;violin&quot;, inner=None, data=train, height = 6, order = ['C', 'Q', 'S'])plt.title('Distribution of Fare by Embarked')plt.tight_layout() Fare vs Pclass1sns.catplot(x=&quot;Pclass&quot;, y=&quot;Fare&quot;, kind=&quot;swarm&quot;, data=train, height = 6) &lt;seaborn.axisgrid.FacetGrid at 0x1fb3698dbe0&gt; 123sns.catplot(x=&quot;Pclass&quot;, y=&quot;Fare&quot;, hue = &quot;Survived&quot;, kind=&quot;swarm&quot;, data=train, palette=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], height = 6)plt.tight_layout() Correlation Matrix12345678910111213plt.figure(figsize=(15,5))plt.subplot(1,2,1)sns.heatmap(train.corr(), annot=True)plt.title('Corelation Matrix');plt.subplot(1,2,2)threshold = 0.3corr = train.corr()sns.heatmap(corr[((corr &gt;= threshold) | (corr &lt;= -threshold)) &amp; (corr != 1)], annot=True, linewidths=.5, fmt= '.2f')plt.title('Configured Corelation Matrix');plt.tight_layout() 데이터 결측치 확인 Age, Cabin, Embarked 결측치 발견 1train.isnull().mean(axis=0) PassengerId 0.000000 Survived 0.000000 Pclass 0.000000 Name 0.000000 Sex 0.000000 Age 0.198653 SibSp 0.000000 Parch 0.000000 Ticket 0.000000 Fare 0.000000 Cabin 0.771044 Embarked 0.002245 dtype: float64 1234plt.style.use('seaborn')plt.figure(figsize=(10,5))sns.heatmap(train.isnull(), yticklabels = False, cmap='plasma')plt.title('Null Values in Training Set') Text(0.5, 1.0, 'Null Values in Training Set') 결측치 채우기 Embarked : 최빈값으로 채우기 Fare: 중앙값으로 채우기 Cabin: NaN -&gt; U(nknown) 12345678train.loc[train.Age.isnull(), 'Age'] = train.groupby(&quot;Pclass&quot;).Age.transform('median')test.loc[test.Age.isnull(), 'Age'] = test.groupby(&quot;Pclass&quot;).Age.transform('median')train['Embarked'] = train['Embarked'].fillna(mode(train['Embarked']))test['Embarked'] = test['Embarked'].fillna(mode(test['Embarked']))train['Fare'] = train.groupby(&quot;Pclass&quot;)['Fare'].transform(lambda x: x.fillna(x.median()))test['Fare'] = test.groupby(&quot;Pclass&quot;)['Fare'].transform(lambda x: x.fillna(x.median()))train['Cabin'] = train['Cabin'].fillna('U')test['Cabin'] = test['Cabin'].fillna('U') 변수 전처리Sex 변환12345678train['Sex'][train['Sex'] == 'male'] = 0train['Sex'][train['Sex'] == 'female'] = 1test['Sex'][test['Sex'] == 'male'] = 0test['Sex'][test['Sex'] == 'female'] = 1train['Sex'] = train['Sex'].astype(int)test['Sex'] = test['Sex'].astype(int) Embarked ont hot encoding12345678910from sklearn.preprocessing import OneHotEncoderencoder = OneHotEncoder()temp = pd.DataFrame(encoder.fit_transform(train[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])train = train.join(temp)# train.drop(columns='Embarked', inplace=True)temp = pd.DataFrame(encoder.transform(test[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])test = test.join(temp)# test.drop(columns='Embarked', inplace=True) Cabin 등급으로 분류12train['Cabin'] = train['Cabin'].map(lambda x:re.compile(&quot;([a-zA-Z])&quot;).search(x).group())test['Cabin'] = test['Cabin'].map(lambda x:re.compile(&quot;([a-zA-Z])&quot;).search(x).group()) 123cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}train['Cabin'] = train['Cabin'].map(cabin_category)test['Cabin'] = test['Cabin'].map(cabin_category) Name 필요 변수 추출12train['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)test['Name'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand = False) 1 12345678910111213141516train.rename(columns={'Name' : 'Title'}, inplace=True)train['Title'] = train['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', 'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')test.rename(columns={'Name' : 'Title'}, inplace=True)test['Title'] = test['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', 'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')encoder = OneHotEncoder()temp = pd.DataFrame(encoder.fit_transform(train[['Title']]).toarray())train = train.join(temp)# train.drop(columns='Title', inplace=True)temp = pd.DataFrame(encoder.transform(test[['Title']]).toarray())test = test.join(temp)# test.drop(columns='Title', inplace=True) SibSp / Parch 전처리12train['familySize'] = train['SibSp'] + train['Parch'] + 1test['familySize'] = test['SibSp'] + test['Parch'] + 1 1234train['isAlone'] = 0test['isAlone'] = 0train.loc[train['familySize'] &gt; 1, 'isAlone'] = 0test.loc[test['familySize'] &gt; 1, 'isAlone'] = 0 Fare 등급12train['FareBin'] = pd.qcut(train['Fare'], 4, duplicates='drop', labels=False)test['FareBin'] = pd.qcut(test['Fare'], 4, duplicates='drop', labels=False) 나이 등급12train['AgeBin'] = pd.cut(train['Age'], 4, duplicates='drop', labels = False)test['AgeBin'] = pd.cut(test['Age'], 4, duplicates='drop', labels = False) PCA1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 24 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Title 891 non-null object 4 Sex 891 non-null object 5 Age 891 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 891 non-null int64 11 Embarked 891 non-null object 12 S 891 non-null float64 13 C 891 non-null float64 14 Q 891 non-null float64 15 0 891 non-null float64 16 1 891 non-null float64 17 2 891 non-null float64 18 3 891 non-null float64 19 4 891 non-null float64 20 familySize 891 non-null int64 21 isAlone 891 non-null int64 22 FareBin 891 non-null int64 23 AgeBin 891 non-null int64 dtypes: float64(10), int64(10), object(4) memory usage: 167.2+ KB 1234567891011121314151617# from sklearn.preprocessing import StandardScaler# from sklearn.decomposition import PCA# columns = train.drop(columns=[&quot;PassengerId&quot;,&quot;Survived&quot;,'Title','Ticket','Cabin','Embarked']).columns# X_train = StandardScaler().fit_transform(train.drop(columns=[&quot;PassengerId&quot;,&quot;Survived&quot;,'Title','Ticket','Cabin','Embarked']))# new_df = pd.DataFrame(X_train, columns=columns)# pca = PCA(n_components = 2)# df_pca = pca.fit_transform(new_df)# plt.figure(figsize =(8, 6))# plt.scatter(df_pca[:, 0], df_pca[:, 1], c = train['Survived'], cmap ='plasma')# plt.xlabel('First Principal Component')# plt.ylabel('Second Principal Component');# plt.tight_layout() 12345678# fig = plt.figure()# ax = fig.gca(projection = '3d')# pca = PCA(n_components = 3)# df_pca = pca.fit_transform(new_df)# ax.scatter(df_pca[:, 0], df_pca[:, 1], df_pca[:, 2], c = target, cmap ='plasma')# plt.tight_layout() 최종 데이터1train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Title Sex Age SibSp Parch Ticket Fare ... Q 0 1 2 3 4 familySize isAlone FareBin AgeBin 0 1 0 3 Mr 0 22.0 1 0 A/5 21171 7.2500 ... 1.0 0.0 0.0 1.0 0.0 0.0 2 0 0 1 1 2 1 1 Mrs 1 38.0 1 0 PC 17599 71.2833 ... 0.0 0.0 0.0 0.0 1.0 0.0 2 0 3 1 2 3 1 3 Miss 1 26.0 0 0 STON/O2. 3101282 7.9250 ... 1.0 0.0 1.0 0.0 0.0 0.0 1 0 1 1 3 4 1 1 Mrs 1 35.0 1 0 113803 53.1000 ... 1.0 0.0 0.0 0.0 1.0 0.0 2 0 3 1 4 5 0 3 Mr 0 35.0 0 0 373450 8.0500 ... 1.0 0.0 0.0 1.0 0.0 0.0 1 0 1 1 5 rows × 24 columns 모델링 테스트 파이프라인 모델 클래스 선언 cross validation 세팅 1234567891011121314151617181920212223#Common Model Algorithmsfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_processfrom xgboost import XGBClassifier#Common Model Helpersfrom sklearn.preprocessing import OneHotEncoder, LabelEncoderfrom sklearn import feature_selectionfrom sklearn import model_selectionfrom sklearn import metrics#Visualizationimport matplotlib as mplimport matplotlib.pyplot as pltimport matplotlib.pylab as pylabimport seaborn as snsfrom pandas.plotting import scatter_matrix#Configure Visualization Defaults#%matplotlib inline = show plots in Jupyter Notebook browser%matplotlib inlinempl.style.use('ggplot')sns.set_style('white')pylab.rcParams['figure.figsize'] = 12,8 분류 알고리즘 클래스 선언12345678910111213141516171819202122232425262728293031323334353637383940414243#Machine Learning Algorithm (MLA) Selection and InitializationMLA = [ #Ensemble Methods ensemble.AdaBoostClassifier(), ensemble.BaggingClassifier(), ensemble.ExtraTreesClassifier(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(), #Gaussian Processes gaussian_process.GaussianProcessClassifier(), #GLM linear_model.LogisticRegressionCV(), linear_model.PassiveAggressiveClassifier(), linear_model.RidgeClassifierCV(), linear_model.SGDClassifier(), linear_model.Perceptron(), #Navies Bayes naive_bayes.BernoulliNB(), naive_bayes.GaussianNB(), #Nearest Neighbor neighbors.KNeighborsClassifier(), #SVM svm.SVC(probability=True), svm.NuSVC(probability=True), svm.LinearSVC(), #Trees tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier(), #Discriminant Analysis discriminant_analysis.LinearDiscriminantAnalysis(), discriminant_analysis.QuadraticDiscriminantAnalysis(), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html XGBClassifier() ] 테스트 1 복수의 분류 알고리즘 테스트 파이프라인 각 알고리즘의 train/test 결과 123456789101112131415161718192021222324252627282930313233343536373839404142434445Target = ['Survived']Input_col = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'S', 'C', 'Q', 0, 1, 2, 3, 4, 'familySize', 'FareBin', 'AgeBin', 'isAlone']data1 = train#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit#note: this is an alternative to train_test_splitcv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%#create table to compare MLA metricsMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']MLA_compare = pd.DataFrame(columns = MLA_columns)#create table to compare MLA predictionsMLA_predict = data1[Target]#index through MLA and save performance to tablerow_index = 0for alg in MLA: #set name and parameters MLA_name = alg.__class__.__name__ MLA_compare.loc[row_index, 'MLA Name'] = MLA_name MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params()) #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate cv_results = model_selection.cross_validate(alg, data1[Input_col], data1[Target], cv = cv_split, return_train_score = True, n_jobs=4) MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean() MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean() MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3 #let's know the worst that can happen! #save MLA predictions - see section 6 for usage alg.fit(data1[Input_col], data1[Target]) MLA_predict[MLA_name] = alg.predict(data1[Input_col]) row_index+=1#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.htmlMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)MLA_compare .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MLA Name MLA Parameters MLA Train Accuracy Mean MLA Test Accuracy Mean MLA Test Accuracy 3*STD MLA Time 3 GradientBoostingClassifier {'ccp_alpha': 0.0, 'criterion': 'friedman_mse'... 0.922097 0.834701 0.0621344 0.103091 6 LogisticRegressionCV {'Cs': 10, 'class_weight': None, 'cv': None, '... 0.83427 0.826866 0.0666397 0.980571 19 LinearDiscriminantAnalysis {'n_components': None, 'priors': None, 'shrink... 0.831461 0.822761 0.0662721 0.0070276 4 RandomForestClassifier {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w... 0.990262 0.818284 0.0621344 0.18732 0 AdaBoostClassifier {'algorithm': 'SAMME.R', 'base_estimator': Non... 0.852622 0.817537 0.0647805 0.0998992 8 RidgeClassifierCV {'alphas': array([ 0.1, 1. , 10. ]), 'class_w... 0.833146 0.817537 0.0591168 0.00956466 21 XGBClassifier {'objective': 'binary:logistic', 'base_score':... 0.983333 0.814179 0.0589788 0.0819842 1 BaggingClassifier {'base_estimator': None, 'bootstrap': True, 'b... 0.97191 0.811567 0.0703088 0.0281174 15 NuSVC {'break_ties': False, 'cache_size': 200, 'clas... 0.827154 0.800373 0.0629757 0.0798487 2 ExtraTreesClassifier {'bootstrap': False, 'ccp_alpha': 0.0, 'class_... 0.990262 0.795896 0.0592438 0.161286 12 GaussianNB {'priors': None, 'var_smoothing': 1e-09} 0.808801 0.794403 0.0744971 0.00702934 11 BernoulliNB {'alpha': 1.0, 'binarize': 0.0, 'class_prior':... 0.79588 0.786567 0.0534039 0.0066855 17 DecisionTreeClassifier {'ccp_alpha': 0.0, 'class_weight': None, 'crit... 0.990262 0.775 0.0709829 0.00528812 18 ExtraTreeClassifier {'ccp_alpha': 0.0, 'class_weight': None, 'crit... 0.990262 0.772388 0.0791537 0.00439491 13 KNeighborsClassifier {'algorithm': 'auto', 'leaf_size': 30, 'metric... 0.798315 0.738806 0.0723727 0.00712559 5 GaussianProcessClassifier {'copy_X_train': True, 'kernel': None, 'max_it... 0.968727 0.733582 0.054885 0.434667 7 PassiveAggressiveClassifier {'C': 1.0, 'average': False, 'class_weight': N... 0.71779 0.715672 0.210245 0.00498023 16 LinearSVC {'C': 1.0, 'class_weight': None, 'dual': True,... 0.726217 0.714925 0.382803 0.0413757 10 Perceptron {'alpha': 0.0001, 'class_weight': None, 'early... 0.690824 0.686567 0.337236 0.00584321 14 SVC {'C': 1.0, 'break_ties': False, 'cache_size': ... 0.688951 0.685075 0.06356 0.0481072 9 SGDClassifier {'alpha': 0.0001, 'average': False, 'class_wei... 0.669101 0.670149 0.276572 0.0061487 20 QuadraticDiscriminantAnalysis {'priors': None, 'reg_param': 0.0, 'store_cova... 0.615169 0.614925 0.0418243 0.00936949 테스트2: Voting 분류 알고리즘 다양한 알고리즘의 결과를 앙상블한 결과 hard voting: 다수결 soft voting: 결과 분포 결합 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950vote_est = [ #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html ('ada', ensemble.AdaBoostClassifier()), ('bc', ensemble.BaggingClassifier()), ('etc',ensemble.ExtraTreesClassifier()), ('gbc', ensemble.GradientBoostingClassifier()), ('rfc', ensemble.RandomForestClassifier()), #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc ('gpc', gaussian_process.GaussianProcessClassifier()), #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression ('lr', linear_model.LogisticRegressionCV()), #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html ('bnb', naive_bayes.BernoulliNB()), ('gnb', naive_bayes.GaussianNB()), #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html ('knn', neighbors.KNeighborsClassifier()), #SVM: http://scikit-learn.org/stable/modules/svm.html ('svc', svm.SVC(probability=True)), #xgboost: http://xgboost.readthedocs.io/en/latest/model.html ('xgb', XGBClassifier())]#Hard Vote or majority rulesvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')vote_hard_cv = model_selection.cross_validate(vote_hard, data1[Input_col], data1[Target], cv = cv_split, return_train_score = True, n_jobs=4)vote_hard.fit(data1[Input_col], data1[Target])print(&quot;Hard Voting Training w/bin score mean: {:.2f}&quot;. format(vote_hard_cv['train_score'].mean()*100))print(&quot;Hard Voting Test w/bin score mean: {:.2f}&quot;. format(vote_hard_cv['test_score'].mean()*100))print(&quot;Hard Voting Test w/bin score 3*std: +/- {:.2f}&quot;. format(vote_hard_cv['test_score'].std()*100*3))print('-'*10)#Soft Vote or weighted probabilitiesvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')vote_soft_cv = model_selection.cross_validate(vote_soft, data1[Input_col], data1[Target], cv = cv_split, return_train_score = True, n_jobs=4)vote_soft.fit(data1[Input_col], data1[Target])print(&quot;Soft Voting Training w/bin score mean: {:.2f}&quot;. format(vote_soft_cv['train_score'].mean()*100))print(&quot;Soft Voting Test w/bin score mean: {:.2f}&quot;. format(vote_soft_cv['test_score'].mean()*100))print(&quot;Soft Voting Test w/bin score 3*std: +/- {:.2f}&quot;. format(vote_soft_cv['test_score'].std()*100*3))print('-'*10) Hard Voting Training w/bin score mean: 94.46 Hard Voting Test w/bin score mean: 83.10 Hard Voting Test w/bin score 3*std: +/- 7.26 ---------- Soft Voting Training w/bin score mean: 94.29 Soft Voting Test w/bin score mean: 83.32 Soft Voting Test w/bin score 3*std: +/- 6.07 ---------- 테스트3: 파라미터 최적화 각 모델의 파라미터 범위 설정 범위 안에서 모델 성능이 가장 좋은 파리미터를 찾음 범위에 따라서 탐색 시간이 증가 GridSearchCV: CV로 파라미터 Grid search vote_est: 파라미터 탐색 후 파라미터가 저장된 모델 클래스를 갖고있음 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140#WARNING: Running is very computational intensive and time expensive.#Code is written for experimental/developmental purposes and not production ready!import time#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.htmlgrid_n_estimator = [10, 50, 100, 300]grid_ratio = [.1, .25, .5, .75, 1.0]grid_learn = [.01, .03, .05, .1, .25]grid_max_depth = [2, 4, 6, 8, 10, None]grid_min_samples = [5, 10, .03, .05, .10]grid_criterion = ['gini', 'entropy']grid_bool = [True, False]grid_seed = [0]grid_param = [ [{ #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html 'n_estimators': grid_n_estimator, #default=50 'learning_rate': grid_learn, #default=1 #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R 'random_state': grid_seed }], [{ #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier 'n_estimators': grid_n_estimator, #default=10 'max_samples': grid_ratio, #default=1.0 'random_state': grid_seed }], [{ #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier 'n_estimators': grid_n_estimator, #default=10 'criterion': grid_criterion, #default=”gini” 'max_depth': grid_max_depth, #default=None 'random_state': grid_seed }], [{ #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier #'loss': ['deviance', 'exponential'], #default=’deviance’ 'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds. 'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds. #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse” 'max_depth': grid_max_depth, #default=3 'random_state': grid_seed }], [{ #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier 'n_estimators': grid_n_estimator, #default=10 'criterion': grid_criterion, #default=”gini” 'max_depth': grid_max_depth, #default=None 'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds. 'random_state': grid_seed }], [{ #GaussianProcessClassifier 'max_iter_predict': grid_n_estimator, #default: 100 'random_state': grid_seed }], [{ #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV 'fit_intercept': grid_bool, #default: True #'penalty': ['l1','l2'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs 'random_state': grid_seed }], [{ #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB 'alpha': grid_ratio, #default: 1.0 }], #GaussianNB - [{}], [{ #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier 'n_neighbors': [1,2,3,4,5,6,7], #default: 5 'weights': ['uniform', 'distance'], #default = ‘uniform’ 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'] }], [{ #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [1,2,3,4,5], #default=1.0 'gamma': grid_ratio, #edfault: auto 'decision_function_shape': ['ovo', 'ovr'], #default:ovr 'probability': [True], 'random_state': grid_seed }], [{ #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html 'learning_rate': grid_learn, #default: .3 'max_depth': [1,2,4,6,8,10], #default 2 'n_estimators': grid_n_estimator, 'seed': grid_seed }] ]start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counterfor clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm #print(param) start = time.perf_counter() best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc', n_jobs=8) best_search.fit(data1[Input_col], data1[Target]) run = time.perf_counter() - start best_param = best_search.best_params_ print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run)) clf[1].set_params(**best_param)run_total = time.perf_counter() - start_totalprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))print('-'*10) The best parameter for AdaBoostClassifier is {'learning_rate': 0.05, 'n_estimators': 300, 'random_state': 0} with a runtime of 15.58 seconds. The best parameter for BaggingClassifier is {'max_samples': 0.5, 'n_estimators': 300, 'random_state': 0} with a runtime of 12.12 seconds. The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 100, 'random_state': 0} with a runtime of 16.44 seconds. The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 9.63 seconds. The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 300, 'oob_score': True, 'random_state': 0} with a runtime of 27.32 seconds. The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 5.10 seconds. The best parameter for LogisticRegressionCV is {'fit_intercept': False, 'random_state': 0, 'solver': 'lbfgs'} with a runtime of 12.16 seconds. The best parameter for BernoulliNB is {'alpha': 0.25} with a runtime of 0.16 seconds. The best parameter for GaussianNB is {} with a runtime of 0.05 seconds. The best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'distance'} with a runtime of 1.74 seconds. The best parameter for SVC is {'C': 2, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 8.65 seconds. The best parameter for XGBClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'seed': 0} with a runtime of 20.32 seconds. Total optimization time was 2.15 minutes. ---------- 테스트4: 최적화된 파라미터 개별 성능 측정 및 테스트 1과 비교123456789101112131415161718192021222324252627282930313233343536373839Target = ['Survived']Input_col = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'S', 'C', 'Q', 0, 1, 2, 3, 4, 'familySize', 'FareBin', 'AgeBin', 'isAlone']#create table to compare MLA metricsMLA_columns_opt = ['MLA Name', 'MLA Parameters opt','MLA Train Accuracy Mean opt', 'MLA Test Accuracy Mean opt', 'MLA Test Accuracy 3*STD opt' ,'MLA Time opt']MLA_compare_opt = pd.DataFrame(columns = MLA_columns_opt)#create table to compare MLA predictionsMLA_predict_opt = data1[Target]#index through MLA and save performance to tablerow_index = 0for algs in vote_est: alg = algs[1] #set name and parameters MLA_name = alg.__class__.__name__ MLA_compare_opt.loc[row_index, 'MLA Name'] = MLA_name MLA_compare_opt.loc[row_index, 'MLA Parameters opt'] = str(alg.get_params()) #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate cv_results = model_selection.cross_validate(alg, data1[Input_col], data1[Target], cv = cv_split, return_train_score = True, n_jobs=8) MLA_compare_opt.loc[row_index, 'MLA Time opt'] = cv_results['fit_time'].mean() MLA_compare_opt.loc[row_index, 'MLA Train Accuracy Mean opt'] = cv_results['train_score'].mean() MLA_compare_opt.loc[row_index, 'MLA Test Accuracy Mean opt'] = cv_results['test_score'].mean() #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets MLA_compare_opt.loc[row_index, 'MLA Test Accuracy 3*STD opt'] = cv_results['test_score'].std()*3 #let's know the worst that can happen! #save MLA predictions - see section 6 for usage alg.fit(data1[Input_col], data1[Target]) MLA_predict_opt[MLA_name] = alg.predict(data1[Input_col]) row_index+=1#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.htmlMLA_compare_opt.sort_values(by = ['MLA Test Accuracy Mean opt'], ascending = False, inplace = True) 1pd.merge(MLA_compare, MLA_compare_opt, on='MLA Name') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MLA Name MLA Parameters MLA Train Accuracy Mean MLA Test Accuracy Mean MLA Test Accuracy 3*STD MLA Time MLA Parameters opt MLA Train Accuracy Mean opt MLA Test Accuracy Mean opt MLA Test Accuracy 3*STD opt MLA Time opt 0 GradientBoostingClassifier {'ccp_alpha': 0.0, 'criterion': 'friedman_mse'... 0.922097 0.834701 0.0621344 0.103091 {'ccp_alpha': 0.0, 'criterion': 'friedman_mse'... 0.890075 0.829104 0.0612713 0.329587 1 LogisticRegressionCV {'Cs': 10, 'class_weight': None, 'cv': None, '... 0.83427 0.826866 0.0666397 0.980571 {'Cs': 10, 'class_weight': None, 'cv': None, '... 0.834457 0.828358 0.0682744 1.14401 2 RandomForestClassifier {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w... 0.990262 0.818284 0.0621344 0.18732 {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w... 0.923034 0.829478 0.0607063 0.937333 3 AdaBoostClassifier {'algorithm': 'SAMME.R', 'base_estimator': Non... 0.852622 0.817537 0.0647805 0.0998992 {'algorithm': 'SAMME.R', 'base_estimator': Non... 0.839139 0.820896 0.0652718 0.869148 4 XGBClassifier {'objective': 'binary:logistic', 'base_score':... 0.983333 0.814179 0.0589788 0.0819842 {'objective': 'binary:logistic', 'base_score':... 0.876404 0.826493 0.0660827 0.146845 5 BaggingClassifier {'base_estimator': None, 'bootstrap': True, 'b... 0.97191 0.811567 0.0703088 0.0281174 {'base_estimator': None, 'bootstrap': True, 'b... 0.941011 0.830597 0.0711856 0.929337 6 ExtraTreesClassifier {'bootstrap': False, 'ccp_alpha': 0.0, 'class_... 0.990262 0.795896 0.0592438 0.161286 {'bootstrap': False, 'ccp_alpha': 0.0, 'class_... 0.901124 0.826493 0.0464923 0.180431 7 GaussianNB {'priors': None, 'var_smoothing': 1e-09} 0.808801 0.794403 0.0744971 0.00702934 {'priors': None, 'var_smoothing': 1e-09} 0.808801 0.794403 0.0744971 0.00517395 8 BernoulliNB {'alpha': 1.0, 'binarize': 0.0, 'class_prior':... 0.79588 0.786567 0.0534039 0.0066855 {'alpha': 0.25, 'binarize': 0.0, 'class_prior'... 0.796067 0.786194 0.0534625 0.00512936 9 KNeighborsClassifier {'algorithm': 'auto', 'leaf_size': 30, 'metric... 0.798315 0.738806 0.0723727 0.00712559 {'algorithm': 'brute', 'leaf_size': 30, 'metri... 0.990262 0.739925 0.0785259 0.00783632 10 GaussianProcessClassifier {'copy_X_train': True, 'kernel': None, 'max_it... 0.968727 0.733582 0.054885 0.434667 {'copy_X_train': True, 'kernel': None, 'max_it... 0.968727 0.733582 0.054885 0.651379 11 SVC {'C': 1.0, 'break_ties': False, 'cache_size': ... 0.688951 0.685075 0.06356 0.0481072 {'C': 2, 'break_ties': False, 'cache_size': 20... 0.94382 0.727985 0.0605824 0.0867202 테스트5: 파라미터 최적화 후 Voting 분류 알고리즘 최적화된 파리미터 저장: vote_est 최적화된 파라미터로 hard / soft vote 모델링 hard vote train 에러: 파라미터 최적화 전보다 감소 -&gt; 오버피팅 감소 test 에러 : 파라미터 최적화 전보다 증가 -&gt; 오버피팅 감소로 인한 성능 향상 soft vote train 에러: 파라미터 최적화 전보다 감소 -&gt; 오버피팅 감소 test 에러 : 파라미터 최적화 전보다 감소 -&gt; 오버피팅 감소가 아닌 전체적 성능 하락으로 판단/ 단 모델 분산 감소 12345678910111213141516171819202122start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter#Hard Vote or majority rules w/Tuned Hyperparametersgrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')grid_hard_cv = model_selection.cross_validate(grid_hard, data1[Input_col], data1[Target], cv = cv_split, return_train_score = True, n_jobs=4)grid_hard.fit(data1[Input_col], data1[Target])print(&quot;Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}&quot;. format(grid_hard_cv['train_score'].mean()*100))print(&quot;Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}&quot;. format(grid_hard_cv['test_score'].mean()*100))print(&quot;Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}&quot;. format(grid_hard_cv['test_score'].std()*100*3))print('-'*10)#Soft Vote or weighted probabilities w/Tuned Hyperparametersgrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')grid_soft_cv = model_selection.cross_validate(grid_soft, data1[Input_col], data1[Target], cv = cv_split, return_train_score = True, n_jobs=4)grid_soft.fit(data1[Input_col], data1[Target])print(&quot;Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}&quot;. format(grid_soft_cv['train_score'].mean()*100))print(&quot;Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}&quot;. format(grid_soft_cv['test_score'].mean()*100))print(&quot;Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}&quot;. format(grid_soft_cv['test_score'].std()*100*3))print('-'*10)run_total = time.perf_counter() - start_totalprint('Total optimization time was {:.2f} minutes.'.format(run_total/60)) Hard Voting w/Tuned Hyperparameters Training w/bin score mean: 90.64 Hard Voting w/Tuned Hyperparameters Test w/bin score mean: 83.21 Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- 5.84 ---------- Soft Voting w/Tuned Hyperparameters Training w/bin score mean: 90.43 Soft Voting w/Tuned Hyperparameters Test w/bin score mean: 82.61 Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- 6.22 ---------- Total optimization time was 0.63 minutes. 최종 모형 최적 파라미터 반영 개별 모델 최적 파라미터 반영 Voting 모델 123456789101112Target = ['Survived']Input_col = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'S', 'C', 'Q', 0, 1, 2, 3, 4, 'familySize', 'FareBin', 'AgeBin', 'isAlone']MLA_predict_sub = gender_submissionfor algs in vote_est: alg = algs[1] MLA_name = alg.__class__.__name__ alg.fit(train[Input_col], train[Target]) MLA_predict_sub[MLA_name] = alg.predict(test[Input_col]) 12MLA_predict_sub['vote hard'] = grid_hard.predict(test[Input_col])MLA_predict_sub['soft hard'] = grid_soft.predict(test[Input_col]) 12345for col_nm in MLA_predict_sub.columns[2:]: sub = pd.DataFrame() sub['PassengerId'] = MLA_predict_sub['PassengerId'] sub['Survived'] = MLA_predict_sub[col_nm] sub.to_csv('sub_'+col_nm+'.csv', index=None) 제출1pd.DataFrame([[col_nm, np.mean(MLA_predict_sub['Survived'] == MLA_predict_sub[col_nm])] for col_nm in MLA_predict_sub.columns[2:]]).sort_values(by=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 10 KNeighborsClassifier 0.650718 6 GaussianProcessClassifier 0.660287 11 SVC 0.674641 2 BaggingClassifier 0.851675 9 GaussianNB 0.887560 5 RandomForestClassifier 0.894737 0 XGBClassifier 0.901914 4 GradientBoostingClassifier 0.904306 13 soft hard 0.904306 12 vote hard 0.911483 3 ExtraTreesClassifier 0.918660 1 AdaBoostClassifier 0.925837 7 LogisticRegressionCV 0.925837 8 BernoulliNB 0.944976 Referencehttps://www.kaggle.com/startupsci/titanic-data-science-solutions","link":"/2020/10/07/ml/Titanic/Titanic/"}],"tags":[{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"XAI","slug":"XAI","link":"/tags/XAI/"},{"name":"Model interpretability","slug":"Model-interpretability","link":"/tags/Model-interpretability/"},{"name":"scikit-learn","slug":"scikit-learn","link":"/tags/scikit-learn/"},{"name":"semi-supervised learning","slug":"semi-supervised-learning","link":"/tags/semi-supervised-learning/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"Delta method","slug":"Delta-method","link":"/tags/Delta-method/"},{"name":"Asymptotical theory","slug":"Asymptotical-theory","link":"/tags/Asymptotical-theory/"},{"name":"Monte carlo","slug":"Monte-carlo","link":"/tags/Monte-carlo/"},{"name":"Markov chain","slug":"Markov-chain","link":"/tags/Markov-chain/"},{"name":"Variable reduction","slug":"Variable-reduction","link":"/tags/Variable-reduction/"},{"name":"Random number generation","slug":"Random-number-generation","link":"/tags/Random-number-generation/"},{"name":"sampling","slug":"sampling","link":"/tags/sampling/"}],"categories":[{"name":"ML","slug":"ML","link":"/categories/ML/"},{"name":"Hexo","slug":"Hexo","link":"/categories/Hexo/"},{"name":"Stats","slug":"Stats","link":"/categories/Stats/"}]}